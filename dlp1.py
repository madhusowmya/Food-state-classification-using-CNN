# -*- coding: utf-8 -*-
"""DLP1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19RdPNlLEZKgOe0ItX2tiB1dkIZDAgHgY
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import os
import cv2
import random

import matplotlib.pyplot as plt
# %matplotlib inline

import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten

from google.colab import drive
drive.mount('/content/drive')

"""DEFINING PATH FOR DATA"""

# Setting base directory path for training and validation 
DATADIR = '/content/drive/My Drive/NewData/train'
VALDATA  = '/content/drive/My Drive/NewData/valid/'
# Setting directory path for images

CATEGORIES= os.listdir(DATADIR)
CATEGORIES.sort()
print(CATEGORIES)

CATEGORIES

# print('Number of images in creamy_paste: ', len(os.listdir(os.path.join(train_dir, 'creamy_paste'))), len(os.listdir(os.path.join(val_dir, 'creamy_paste'))))
# print('Number of images in diced: ', len(os.listdir(os.path.join(train_dir, 'diced'))), len(os.listdir(os.path.join(val_dir, 'diced'))))
# print('Number of images in floured: ', len(os.listdir(os.path.join(train_dir, 'floured'))), len(os.listdir(os.path.join(val_dir, 'floured'))))
# print('Number of images in grated: ', len(os.listdir(os.path.join(train_dir, 'grated'))), len(os.listdir(os.path.join(val_dir, 'grated'))))
# print('Number of images in juiced: ', len(os.listdir(os.path.join(train_dir, 'juiced'))), len(os.listdir(os.path.join(val_dir, 'juiced'))))
# print('Number of images in jullienne: ', len(os.listdir(os.path.join(train_dir, 'jullienne'))), len(os.listdir(os.path.join(val_dir, 'jullienne'))))
# print('Number of images in mixed: ', len(os.listdir(os.path.join(train_dir, 'mixed'))), len(os.listdir(os.path.join(val_dir, 'mixed'))))
# print('Number of images in other: ', len(os.listdir(os.path.join(train_dir, 'other'))), len(os.listdir(os.path.join(val_dir, 'other'))))
# print('Number of images in peeled: ', len(os.listdir(os.path.join(train_dir, 'peeled'))), len(os.listdir(os.path.join(val_dir, 'peeled'))))
# print('Number of images in sliced: ', len(os.listdir(os.path.join(train_dir, 'sliced'))), len(os.listdir(os.path.join(val_dir, 'sliced'))))
# print('Number of images in whole: ', len(os.listdir(os.path.join(train_dir, 'whole'))), len(os.listdir(os.path.join(val_dir, 'whole'))))

"""DATA PREPROCESSING 

"""

IMG_SIZE = 32


# CATEGORIES = ['creamy_paste', 'diced', 'grated', 'floured', 'juiced','jullienne', 'mixed','peeled', 'sliced', 'other', 'whole']

for category in CATEGORIES :
    path_tr = os.path.join(DATADIR, category)
    path_val = os.path.join(VALDATA, category)
  #   for img in os.listdir(path_tr):
  #       #img_array_tr = cv2.imread(os.path.join(path_tr, img), cv2.IMREAD_UNCHANGED)
       
  #  for img in os.listdir(path_val):
  #       #img_array_val = cv2.imread(os.path.join(path_val, img), cv2.IMREAD_UNCHANGED)


training_data = []
validation_data = []

def processing_data():
    for category in CATEGORIES :
        path_tr = os.path.join(DATADIR, category)
        path_val = os.path.join(VALDATA, category)
        class_num = CATEGORIES.index(category)
        print(class_num)
        for img in os.listdir(path_tr):
            try :
                img_array_tr = cv2.imread(os.path.join(path_tr, img), cv2.IMREAD_UNCHANGED)
                new_arr_tr = cv2.cvtColor(img_array_tr, cv2.COLOR_BGR2RGB )
                new_array_tr = cv2.resize(new_arr_tr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)
                
                training_data.append([new_array_tr, class_num])
      
            except Exception as e:
                pass
        
        for img in os.listdir(path_val):
          try:
                img_array_val = cv2.imread(os.path.join(path_val, img), cv2.IMREAD_UNCHANGED)
                new_arr_val = cv2.cvtColor(img_array_val, cv2.COLOR_BGR2RGB )
                new_array_val = cv2.resize(new_arr_val, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)
              
                validation_data.append([new_array_val, class_num])
          except Exception as e:
                pass
        print(category)
                # image_arr = cv2.imread(image, cv2.IMREAD_UNCHANGED) # 0 grey scale
                # image_arr = cv2.cvtColor(image_arr, cv2.COLOR_BGR2RGB )
        # X.append(cv2.resize(image_arr ,  (width, height), interpolation=cv2.INTER_AREA))
        # Y.append(label) # minmax



processing_data()
#Normalize the data

# le=LabelEncoder()
# Y=le.fit_transform(Z)
# Y=to_categorical(Y,11)
# X=np.array(X)
# X=X/255

# from tempfile import Temporaryfile()
# outfile = TemporaryFile()
# np.save(outfile,training_data)

# random.shuffle(training_data)
# random.shuffle(validation_data)
X_tr = [] #features
y_tr = [] #labels 
print(X_tr)

from keras.utils import to_categorical
for features, label in training_data:
    
    X_tr.append(features)
    #y_tr = y_tr.tolist()
    y_tr.append(label)
    #y_tr = to_categorical(y_tr)

#X_tr = np.array(X_tr).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
# X_tr = X_tr/255
X_tr = np.array(X_tr)
y_tr = np.asarray(y_tr)

y_tr.shape

from keras.utils import to_categorical

# Shuffle both inputs and labels for splitting in the future.
shuffler = np.random.permutation(len(X_tr))
X_tr = X_tr[shuffler]
y_tr = y_tr[shuffler]

y_tr = to_categorical(y_tr)
x_train = X_tr
y_train = y_tr
print('Shape of train images: ', x_train.shape)
print('Shape of train labels: ', y_train.shape)



X_val = []
y_val = []

for features, label in validation_data:
    X_val.append(features)
    # y_val = y_val.tolist()
    y_val.append(label)
    #y_val = to_categorical(y_val)
X_val = np.array(X_val)
#X_val = np.array(X_val).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
#X_val = X_val/255
y_val = np.asarray(y_val)

y_val.shape

from keras.utils import to_categorical

# Shuffle both inputs and labels for splitting in the future.
shuffler = np.random.permutation(len(X_val))
X_val = X_val[shuffler]
y_val = y_val[shuffler]

y_val = to_categorical(y_val)
x_valid = X_val
y_valid = y_val
print('Shape of validation images: ', x_valid.shape)
print('Shape of validation labels: ', y_valid.shape)

"""NORAMLIZATION AND AUGMENTATION"""

train_gen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        zca_epsilon=1e-06,  # epsilon for ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        # randomly shift images horizontally (fraction of total width)
        width_shift_range=0.1,
        # randomly shift images vertically (fraction of total height)
        height_shift_range=0.1,
        shear_range=0.3,  
        zoom_range=0.3,  
        channel_shift_range=0.2,  
        fill_mode='nearest',
        cval=0.2,  
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True,  # randomly flip images
        rescale=None,
        preprocessing_function=None,
        data_format=None)
# horizontal_flip = True

# Validation dataset doesn't need image augmentation. Only normalization
val_datagen = ImageDataGenerator(rescale=1./255)

batch_size = 16

from tensorflow.keras import regularizers, initializers

train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)
val_generator = val_datagen.flow(x_valid, y_valid, batch_size=batch_size)

# from keras.layers import BatchNormalization, Activation
# model = Sequential()
# wd = 0.001
# # First convolution as the input layer with relu activation. 
# model.add(Conv2D(256, (3,3), activation='relu', input_shape=(32,32,3), kernel_regularizer=regularizers.l2(wd)))
# model.add(BatchNormalization())
# model.add(MaxPooling2D((2, 2)))

# # Second convolution with relu activation and 64 output filters
# model.add(Conv2D(512, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(wd),padding='SAME'))
# model.add(BatchNormalization())
# model.add(MaxPooling2D((2, 2)))

# # Third convolution with relu activation and 128 output filters
# model.add(Conv2D(512, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(wd)))
# model.add(BatchNormalization())
# model.add(MaxPooling2D((2, 2)))

# model.add(Conv2D(512, (3, 3), activation='relu', padding='SAME', kernel_regularizer=regularizers.l2(wd)))
# model.add(BatchNormalization())
# model.add(MaxPooling2D((2, 2)))

# # model.add(Conv2D(512, (3, 3), activation='relu', padding='SAME', kernel_regularizer=regularizers.l2(wd)))
# # model.add(MaxPooling2D((2, 2)))

# # Flatten the output layer to 1 dimension
# model.add(Flatten())

# # Add a dropout rate of 0.5 Dropout randomly drops some layers in a neural networks and then learns with the reduced network.
# # model.add(Dropout(0.5))

# # Add a fully connected layer with 100 hidden units and ReLU activation
# model.add(Dense(4096, activation='relu'))
# model.add(Dropout(0.5))
# # model.add(Dense(1024, activation='relu'))
# # model.add(Dropout(0.5))

# # Add a final softmax classification with 3 hidden units for 3 classes
# model.add(Dense(11, activation='softmax'))
# model.summary()

"""MODEL ARCHITECTURE"""

#model defining for CNN 1st level
from keras.layers import GaussianNoise
model = Sequential()
model.add(Conv2D(512, (3, 3), padding='same',
                 input_shape=(32,32,3)))
model.add(GaussianNoise(0.1))
model.add(Activation('relu'))
model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(256, (3, 3), padding='same'))
model.add(GaussianNoise(0.1))
model.add(Activation('relu'))
model.add(Conv2D(512, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(11))
model.add(Activation('softmax'))



# model = Sequential()
# from keras.layers import BatchNormalization, Activation
# # First convolution as the input layer with relu activation. 
# model.add(Conv2D(128, (3, 3), activation='relu', input_shape=(32,32,3)))
# model.add(MaxPooling2D((2, 2)))
# model.add(BatchNormalization())



# # Second convolution with relu activation and 256 output filters
# model.add(Conv2D(128, (3, 3), activation='relu',padding='same' ))
# model.add(MaxPooling2D((2, 2)))


# # Third convolution with relu activation and 512 output filters
# model.add(Conv2D(256, (3, 3), activation='relu'))
# model.add(MaxPooling2D((2, 2)))
# model.add(BatchNormalization())


# # Fifth convolution with relu activation and 512 output filters
# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
# model.add(MaxPooling2D((2, 2)))


# # Flatten the output layer to 1 dimension
# model.add(Flatten())

# # Add a dropout rate of 0.5 Dropout randomly drops some layers in a neural networks and then learns with the reduced network.
# model.add(Dropout(0.5))

# # Add a fully connected layer with 100 hidden units and ReLU activation
# model.add(Dense(512, activation='relu'))

# # Add a final softmax classification with 3 hidden units for 3 classes
# model.add(Dense(11, activation='softmax'))

# model.summary()

# Length of train and validation data
ntrain = len(x_train)
nvalid = len(x_valid)
nvalid

from tensorflow.keras.optimizers import RMSprop

# initiate adam optimizer with learning rate 0.0001
opt = keras.optimizers.Adam(lr=0.001)

# Compiling the model
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

import tensorflow as tf
checkpoint_path = '/content/drive/My Drive/NewData/'
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)


# Compiling the model
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
# Training model
history = model.fit(train_generator,
                            steps_per_epoch= ntrain//batch_size,
                             epochs=300,
                             validation_data= val_generator,
                              validation_steps= nvalid//batch_size,callbacks=[cp_callback])



ntrain = len(x_train)
nvalid = len(x_valid)
ntrain

# model = Sequential()
# from keras.layers import BatchNormalization, Activation
# # First convolution as the input layer with relu activation. 
# model.add(Conv2D(1024, (3, 3), activation='relu', input_shape=(32,32,3)))
# model.add(MaxPooling2D((2, 2)))
# model.add(BatchNormalization())



# # # Second convolution with relu activation and 256 output filters
# # model.add(Conv2D(128, (3, 3), activation='relu',padding='same' ))
# # model.add(MaxPooling2D((2, 2)))


# # Third convolution with relu activation and 512 output filters
# model.add(Conv2D(256, (3, 3), activation='relu'))
# model.add(MaxPooling2D((2, 2)))
# model.add(BatchNormalization())


# # Fifth convolution with relu activation and 512 output filters
# model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
# model.add(MaxPooling2D((2, 2)))


# # Flatten the output layer to 1 dimension
# model.add(Flatten())

# # Add a dropout rate of 0.5 Dropout randomly drops some layers in a neural networks and then learns with the reduced network.
# model.add(Dropout(0.5))

# # Add a fully connected layer with 100 hidden units and ReLU activation
# model.add(Dense(100, activation='relu'))

# # Add a final softmax classification with 3 hidden units for 3 classes
# model.add(Dense(11, activation='softmax'))

# model.summary()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

# Train and validation accuracy
plt.plot(epochs, acc, 'b', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

# Train and validation loss
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()





"""TEST RUNNING CODE"""

#test images folder path

TESTDATA  = '/content/drive/My Drive/NewData/test_anonymous/' #images path
# Setting directory path for test images

width = 32
height = 32

test_images = [TESTDATA + '/{}'.format(i) for i in os.listdir(TESTDATA)]

def read_process_images(image_list):
  T = []
  for image in image_list:
    image_arr = cv2.imread(image, cv2.IMREAD_UNCHANGED)
    image_arr = cv2.cvtColor(image_arr, cv2.COLOR_BGR2RGB )
    T.append(cv2.resize(image_arr ,  (width, height), interpolation=cv2.INTER_AREA))
  return T

test = read_process_images(test_images)
# test
x_test = np.array(test)

x_test.shape

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow(x_test, batch_size=batch_size)

y_pred = np.argmax(model.predict(x_test), axis=-1)

y_pred

"""JSON FILE GENERATION

"""

# initializing lists 
image_list = []
import os
     # 1.Get file names from directory
image_list=os.listdir(r"/content/drive/My Drive/NewData/test_anonymous/")
image_list.sort()
print(image_list)

file_list = []
for i in range(len(image_list)):
  x = 'test_anonymous/'
  y = image_list[i]
  z = x + y
  file_list.append(z)

print(file_list)

D = []
for i in range(len(y_pred)):
  D.append((CATEGORIES[y_pred[i]]))
print(D)

"""Dictionary with IMAGE file names as keys and Predicted class as values

"""

# Printing original keys-value lists 
print ("Original key list is : " + str(file_list)) 
print ("Original value list is : " + str(D)) 
  
## using dictionary comprehension 
# to convert lists to dictionary 
res = {file_list[i]: D[i] for i in range(len(file_list))} 
  
# Printing resultant dictionary  
print ("Resultant dictionary is : " +  str(res))

len(res)

from google.colab import files
import json

x = json.dumps (res)
with open('MadhuSowmya.json', 'w') as f:
  f.write(x)

files.download('MadhuSowmya.json')

y

test={}
for i in res.values():
  test[i]= 1
test







